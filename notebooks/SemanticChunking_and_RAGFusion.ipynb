{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain\n",
        "!pip install langchain_openai langchain_experimental\n",
        "!pip install langchainhub\n",
        "!pip install langchain_together\n",
        "!pip -q install --upgrade together\n",
        "!pip install fastembed\n",
        "!pip install qdrant-client\n",
        "!pip install sentence_transformers\n",
        "!pip install langchain-cohere\n",
        "!pip install einops\n",
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "6Ui17RdZF-Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TOGETHER_API_KEY\"] = \"148521c4088ad416dced465cc144671626b00c860af4e6ebc855953567087d8a\"\n",
        "os.environ[\"COHERE_API_KEY\"] = 'xxe3X6u8vcTFJgJ8Pc7CfLezwpQiATQcUB56VIUp'\n",
        "\n",
        "TOP_K = 5\n",
        "MAX_DOCS_FOR_CONTEXT = 5"
      ],
      "metadata": {
        "id": "ZEelWy8VH1l3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain.vectorstores import Qdrant\n",
        "from qdrant_client import QdrantClient\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from qdrant_client.http import models\n",
        "from langchain_core.documents.base import Document\n",
        "from langchain.load import dumps, loads\n",
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
        "from langchain_cohere.llms import Cohere\n",
        "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
        "from sentence_transformers.util import cos_sim\n",
        "import PyPDF2\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "xnd9ZYqnc2MB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "semantic_chunker_embed_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")"
      ],
      "metadata": {
        "id": "7TbUEh-0Azxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qdrant_client = QdrantClient(\n",
        "    \"https://8803fa99-7551-4f88-84c3-e134c9bed5de.us-east4-0.gcp.cloud.qdrant.io:6333\",\n",
        "    prefer_grpc=True,\n",
        "    api_key=\"EFeN_UhdmAlDNYZHqJBUbZ88Nt7N0MkmvWLgM5Hs4ogNvExLMwNwdQ\",\n",
        ")"
      ],
      "metadata": {
        "id": "V99hGlj2cnkA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_QDrant_collection():\n",
        "\t\tembedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\t\tqdrant_client.recreate_collection(\n",
        "\t\tcollection_name=\"owners_manual\",\n",
        "\t\tvectors_config=models.VectorParams(\n",
        "\t\t\tsize=embedding_model.get_sentence_embedding_dimension(),\n",
        "\t\t\tdistance=models.Distance.COSINE\n",
        "\t\t)\n",
        "\t)\n",
        "\n",
        "create_QDrant_collection()"
      ],
      "metadata": {
        "id": "BAbk73KXdO-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42a22176-3224-4640-c60a-a65ef99297d9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-74b9a7dde14d>:3: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
            "  qdrant_client.recreate_collection(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_chunks_to_QDrant(documents):\n",
        "  records_to_upload = []\n",
        "  for idx, chunk in enumerate(documents):\n",
        "      content = chunk.page_content\n",
        "      vector = embedding_model.encode(content).tolist()\n",
        "\n",
        "      record = models.PointStruct(\n",
        "          id=idx,\n",
        "          vector=vector,\n",
        "          payload={\"page_content\": content}\n",
        "      )\n",
        "      records_to_upload.append(record)\n",
        "\n",
        "  qdrant_client.upload_points(\n",
        "      collection_name=\"owners_manual\",\n",
        "      points=records_to_upload\n",
        "  )"
      ],
      "metadata": {
        "id": "pj2OjJQ_fbnq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_pdf_to_QDrant(pdf_path):\n",
        "    semantic_chunker = SemanticChunker(\n",
        "        semantic_chunker_embed_model, breakpoint_threshold_type=\"percentile\"\n",
        "    )\n",
        "\n",
        "    pdf_file_path = pdf_path\n",
        "    pdf_file = open(pdf_file_path, 'rb')\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "\n",
        "    text = ''\n",
        "    documents = []\n",
        "    for page_num in range(len(pdf_reader.pages)):\n",
        "        page = pdf_reader.pages[page_num]\n",
        "        text = page.extract_text()\n",
        "        chunks = semantic_chunker.create_documents([text])\n",
        "        documents.append(chunks)\n",
        "\n",
        "    upload_chunks_to_QDrant(documents)\n",
        "    pdf_file.close()"
      ],
      "metadata": {
        "id": "PucnNRleX7Kf"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upload_pdf_to_QDrant(\"/content/text.pdf\")"
      ],
      "metadata": {
        "id": "XPLFoiCHYTJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Please answer the [question] using only the following [information]. If there is no [information] available to answer the question, do not force an answer.\n",
        "\n",
        "Information: {context}\n",
        "\n",
        "Question: {question}\n",
        "Final answer:\"\"\""
      ],
      "metadata": {
        "id": "syOqunU1N1fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reciprocal_rank_fusion(results: list[list], k=60):\n",
        "    \"\"\"Rerank docs (Reciprocal Rank Fusion)\n",
        "\n",
        "    Args:\n",
        "        results (list[list]): retrieved documents\n",
        "        k (int, optional): parameter k for RRF. Defaults to 60.\n",
        "\n",
        "    Returns:\n",
        "        ranked_results: list of documents reranked by RRF\n",
        "    \"\"\"\n",
        "\n",
        "    fused_scores = {}\n",
        "    for docs in results:\n",
        "        # Assumes the docs are returned in sorted order of relevance\n",
        "        for rank, doc in enumerate(docs):\n",
        "            doc_str = dumps(doc)\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "\n",
        "    # return only documents\n",
        "    return [x[0] for x in reranked_results[:MAX_DOCS_FOR_CONTEXT]]"
      ],
      "metadata": {
        "id": "RMDsb0oNHf2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_generator(original_query: dict) -> list[str]:\n",
        "    \"\"\"Generate queries from original query\n",
        "\n",
        "    Args:\n",
        "        query (dict): original query\n",
        "\n",
        "    Returns:\n",
        "        list[str]: list of generated queries\n",
        "    \"\"\"\n",
        "\n",
        "    # original query\n",
        "    query = original_query.get(\"query\")\n",
        "\n",
        "    # prompt for query generator\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a helpful assistant that generates multiple search queries based on a single input query.\"),\n",
        "        (\"user\", \"Generate multiple search queries related to:  {original_query}. When creating queries, please refine or add closely related contextual information, without significantly altering the original query's meaning\"),\n",
        "        (\"user\", \"OUTPUT (3 queries):\")\n",
        "    ])\n",
        "\n",
        "    # LLM model\n",
        "    model = Cohere()\n",
        "\n",
        "    # query generator chain\n",
        "    query_generator_chain = (\n",
        "        prompt | model | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
        "    )\n",
        "\n",
        "    # gererate queries\n",
        "    queries = query_generator_chain.invoke({\"original_query\": query})\n",
        "\n",
        "    # add original query\n",
        "    queries.insert(0, \"0. \" + query)\n",
        "\n",
        "    return queries"
      ],
      "metadata": {
        "id": "EMDHKjwZITwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rrf_retriever(query: str) -> list[Document]:\n",
        "    \"\"\"RRF retriever\n",
        "\n",
        "    Args:\n",
        "        query (str): Query string\n",
        "\n",
        "    Returns:\n",
        "        list[Document]: retrieved documents\n",
        "    \"\"\"\n",
        "\n",
        "    # Retriever\n",
        "    embedding = SentenceTransformerEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "    qdrant = Qdrant(\n",
        "        client=qdrant_client,\n",
        "        collection_name=\"owners_manual\",\n",
        "        embeddings=embedding,\n",
        "    )\n",
        "\n",
        "    retriever = qdrant.as_retriever()\n",
        "\n",
        "    # RRF chain\n",
        "    chain = (\n",
        "        {\"query\": itemgetter(\"query\")}\n",
        "        | RunnableLambda(query_generator)\n",
        "        | retriever.map()\n",
        "        | reciprocal_rank_fusion\n",
        "    )\n",
        "\n",
        "    # invoke\n",
        "    result = chain.invoke({\"query\": query})\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "4LGJuyflIc6j"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query(query: str, retriever: BaseRetriever):\n",
        "    \"\"\"\n",
        "    Query with vectordb\n",
        "    \"\"\"\n",
        "\n",
        "    # model\n",
        "    model = Cohere()\n",
        "\n",
        "    # prompt\n",
        "    prompt = PromptTemplate(\n",
        "        template=template,\n",
        "        input_variables=[\"context\", \"question\"],\n",
        "    )\n",
        "\n",
        "    # Query chain\n",
        "    chain = (\n",
        "        {\n",
        "            \"context\": itemgetter(\"question\") | retriever,\n",
        "            \"question\": itemgetter(\"question\")\n",
        "        }\n",
        "        | RunnablePassthrough.assign(\n",
        "            context=itemgetter(\"context\")\n",
        "        )\n",
        "        | {\n",
        "            \"response\": prompt | model | StrOutputParser(),\n",
        "            \"context\": itemgetter(\"context\"),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # execute chain\n",
        "    result = chain.invoke({\"question\": query})\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "mgQTzKAnIw80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = RunnableLambda(rrf_retriever)\n",
        "result = query(\"Enter Query Here\", retriever)\n",
        "result['response']"
      ],
      "metadata": {
        "id": "ZPgHLHD7VcZo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}